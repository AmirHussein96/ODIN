{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Domain_adaptation_toy _example.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpKVOtC_4Sue"
      },
      "source": [
        "!git clone https://github.com/pumpikano/tf-dann"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzRcwizP4YK2"
      },
      "source": [
        "cd 'tf-dann/'\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OApQClD7F4K"
      },
      "source": [
        "%tensorflow_version 1.14\n",
        "from functools import partial\n",
        "def compute_pairwise_distances(x, y):\n",
        "  \"\"\"Computes the squared pairwise Euclidean distances between x and y.\n",
        "  Args:\n",
        "    x: a tensor of shape [num_x_samples, num_features]\n",
        "    y: a tensor of shape [num_y_samples, num_features]\n",
        "  Returns:\n",
        "    a distance matrix of dimensions [num_x_samples, num_y_samples].\n",
        "  Raises:\n",
        "    ValueError: if the inputs do no matched the specified dimensions.\n",
        "  \"\"\"\n",
        "\n",
        "  if not len(x.get_shape()) == len(y.get_shape()) == 2:\n",
        "    raise ValueError('Both inputs should be matrices.')\n",
        "\n",
        "  if x.get_shape().as_list()[1] != y.get_shape().as_list()[1]:\n",
        "    raise ValueError('The number of features should be the same.')\n",
        "\n",
        "  norm = lambda x: tf.reduce_sum(tf.square(x), 1)\n",
        "\n",
        "  # By making the `inner' dimensions of the two matrices equal to 1 using\n",
        "  # broadcasting then we are essentially substracting every pair of rows\n",
        "  # of x and y.\n",
        "  # x will be num_samples x num_features x 1,\n",
        "  # and y will be 1 x num_features x num_samples (after broadcasting).\n",
        "  # After the substraction we will get a\n",
        "  # num_x_samples x num_features x num_y_samples matrix.\n",
        "  # The resulting dist will be of shape num_y_samples x num_x_samples.\n",
        "  # and thus we need to transpose it again.\n",
        "  return tf.transpose(norm(tf.expand_dims(x, 2) - tf.transpose(y)))\n",
        "  \n",
        "def gaussian_kernel_matrix(x, y, sigmas):\n",
        "  r\"\"\"Computes a Guassian Radial Basis Kernel between the samples of x and y.\n",
        "  We create a sum of multiple gaussian kernels each having a width sigma_i.\n",
        "  Args:\n",
        "    x: a tensor of shape [num_samples, num_features]\n",
        "    y: a tensor of shape [num_samples, num_features]\n",
        "    sigmas: a tensor of floats which denote the widths of each of the\n",
        "      gaussians in the kernel.\n",
        "  Returns:\n",
        "    A tensor of shape [num_samples{x}, num_samples{y}] with the RBF kernel.\n",
        "  \"\"\"\n",
        "  beta = 1. / (2. * (tf.expand_dims(sigmas, 1)))\n",
        "\n",
        "  dist = compute_pairwise_distances(x, y)\n",
        "\n",
        "  s = tf.matmul(beta, tf.reshape(dist, (1, -1)))\n",
        "  return tf.reshape(tf.reduce_sum(tf.exp(-s), 0), tf.shape(dist))\n",
        "\n",
        "def normalize(x, mean = None, std = None):\n",
        "\n",
        "    x_reshaped = tf.reshape(x,[-1,3])\n",
        "  \n",
        "        # mean of each column/sensor\n",
        "    x_reshaped = (x_reshaped-mean)/std\n",
        "    # reshape back to original shape\n",
        "    return tf.reshape(x_reshaped,[-1,128,3])\n",
        "\n",
        "\n",
        "def fixprob(att):\n",
        "    att = att + 1e-9\n",
        "    _sum = tf.reduce_sum(att, reduction_indices=1, keep_dims=True)\n",
        "    att = att / _sum\n",
        "    att = tf.clip_by_value(att, 1e-9, 1.0, name=None)\n",
        "    return att\n",
        "\n",
        "def maximum_mean_discrepancy(x, y, kernel=gaussian_kernel_matrix):\n",
        "  r\"\"\"Computes the Maximum Mean Discrepancy (MMD) of two samples: x and y.\n",
        "  Maximum Mean Discrepancy (MMD) is a distance-measure between the samples of\n",
        "  the distributions of x and y. Here we use the kernel two sample estimate\n",
        "  using the empirical mean of the two distributions.\n",
        "  MMD^2(P, Q) = || \\E{\\phi(x)} - \\E{\\phi(y)} ||^2\n",
        "              = \\E{ K(x, x) } + \\E{ K(y, y) } - 2 \\E{ K(x, y) },\n",
        "  where K = <\\phi(x), \\phi(y)>,\n",
        "    is the desired kernel function, in this case a radial basis kernel.\n",
        "  Args:\n",
        "      x: a tensor of shape [num_samples, num_features]\n",
        "      y: a tensor of shape [num_samples, num_features]\n",
        "      kernel: a function which computes the kernel in MMD. Defaults to the\n",
        "              GaussianKernelMatrix.\n",
        "  Returns:\n",
        "      a scalar denoting the squared maximum mean discrepancy loss.\n",
        "  \"\"\"\n",
        "  with tf.name_scope('MaximumMeanDiscrepancy'):\n",
        "    # \\E{ K(x, x) } + \\E{ K(y, y) } - 2 \\E{ K(x, y) }\n",
        "    cost = tf.reduce_mean(kernel(x, x))\n",
        "    cost += tf.reduce_mean(kernel(y, y))\n",
        "    cost -= 2 * tf.reduce_mean(kernel(x, y))\n",
        "\n",
        "    # We do not allow the loss to become negative.\n",
        "    cost = tf.where(cost > 0, cost, 0, name='value')\n",
        "  return cost\n",
        "\n",
        "\n",
        "def mmd_loss(source_samples, target_samples, weight, scope=None):\n",
        "  \"\"\"Adds a similarity loss term, the MMD between two representations.\n",
        "  This Maximum Mean Discrepancy (MMD) loss is calculated with a number of\n",
        "  different Gaussian kernels.\n",
        "  Args:\n",
        "    source_samples: a tensor of shape [num_samples, num_features].\n",
        "    target_samples: a tensor of shape [num_samples, num_features].\n",
        "    weight: the weight of the MMD loss.\n",
        "    scope: optional name scope for summary tags.\n",
        "  Returns:\n",
        "    a scalar tensor representing the MMD loss value.\n",
        "  \"\"\"\n",
        "  sigmas = [\n",
        "      1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 5, 10, 15, 20, 25, 30, 35, 100,\n",
        "      1e3, 1e4, 1e5, 1e6\n",
        "  ]\n",
        "  gaussian_kernel = partial(\n",
        "      gaussian_kernel_matrix, sigmas=tf.constant(sigmas))\n",
        "\n",
        "  loss_value = maximum_mean_discrepancy(\n",
        "      source_samples, target_samples, kernel=gaussian_kernel)\n",
        "  loss_value = tf.maximum(1e-4, loss_value) * weight\n",
        "  return loss_value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qn5AVPz34g7X"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "from sklearn.datasets import make_moons, make_blobs\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras import layers\n",
        "from flip_gradient import flip_gradient\n",
        "from sklearn.model_selection import train_test_split\n",
        "from utils import *\n",
        "\n",
        "\n",
        "\n",
        "# #%% case1 + overlap\n",
        "\n",
        "# Xs, ys = make_blobs(1000, centers=[[0, 1], [-0.2, 2]], cluster_std=[0.35,0.3])\n",
        "# Xt, yt = make_blobs(1000, centers=[[0.7, -0.2], [0.6, -1.3]], cluster_std=[0.3,0.4])\n",
        "\n",
        "# plt.scatter(Xs[:,0], Xs[:,1], c=ys, cmap='coolwarm', alpha=0.4)\n",
        "# plt.scatter(Xt[:,0], Xt[:,1], c=yt, cmap='cool', alpha=0.4)\n",
        "# plt.title('Source domain and target domain blobs data',fontsize=14,fontweight='bold')\n",
        "# X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(Xs, ys, test_size=0.3, random_state=42)\n",
        "# X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(Xt, yt, test_size=0.3, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-8vARYI-nqs"
      },
      "source": [
        "\n",
        "#%% case1 + overlap\n",
        "\n",
        "Xs, ys = make_blobs(1000, centers=[[0, 1], [-0.2, 2]], cluster_std=[0.35,0.3])\n",
        "Xt, yt = make_blobs(1000, centers=[[0.6, -1.3], [0.7, -0.2]], cluster_std=[0.3,0.4])\n",
        "\n",
        "plt.scatter(Xs[:,0], Xs[:,1], c=ys, cmap='coolwarm', alpha=0.4)\n",
        "plt.scatter(Xt[:,0], Xt[:,1], c=yt, cmap='cool', alpha=0.4)\n",
        "plt.title('Source domain and target domain blobs data',fontsize=14,fontweight='bold')\n",
        "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(Xs, ys, test_size=0.3, random_state=42)\n",
        "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(Xt, yt, test_size=0.3, random_state=42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JawXSzaAyC9s"
      },
      "source": [
        "# #%% case2+overlap\n",
        "\n",
        "# Xs, ys = make_blobs(1000, centers=[[0, 0.5], [-0.2, 2]], cluster_std=[0.35,0.3])\n",
        "# Xt, yt = make_blobs(1000, centers=[[1.5, 2], [1.7, 0.5]], cluster_std=[0.3,0.4])\n",
        "\n",
        "# plt.scatter(Xs[:,0], Xs[:,1], c=ys, cmap='coolwarm', alpha=0.4)\n",
        "# plt.scatter(Xt[:,0], Xt[:,1], c=yt, cmap='cool', alpha=0.4)\n",
        "# plt.title('Source domain and target domain blobs data',fontsize=14,fontweight='bold')\n",
        "# X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(Xs, ys, test_size=0.3, random_state=42)\n",
        "# X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(Xt, yt, test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAFxKkWP4t-R"
      },
      "source": [
        "#%%\n",
        "\n",
        "def fc_layer(input, size_in, size_out, name=\"fc\",init_val=None,tr=True):\n",
        "  \n",
        "  with tf.name_scope(name):\n",
        "   if init_val==None:\n",
        "      \n",
        "        w = tf.get_variable(name=\"W%s\"%(name),initializer=tf.truncated_normal([size_in, size_out], stddev=0.1),trainable=tr)\n",
        "        b = tf.get_variable(initializer=tf.constant_initializer(0.1),name=\"B%s\"%(name),shape=[size_out],trainable=tr)\n",
        "        \n",
        "   else:\n",
        "        w = tf.get_variable(name=\"W%s\"%(name),shape=[size_in, size_out],initializer=tf.constant_initializer(init_val[0]),trainable=tr)\n",
        "        b = tf.get_variable(initializer=tf.constant_initializer(init_val[1]),name=\"B%s\"%(name),shape=[size_out],trainable=tr)\n",
        "   act = tf.matmul(input, w) + b\n",
        "\n",
        "   return act\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "class DANN:\n",
        "    \"\"\"DANN domain adaptation model.\"\"\"\n",
        "    def __init__(self,loss,activation):\n",
        "        self.loss=loss\n",
        "        self.build_model(activation)\n",
        "        \n",
        "    def build_model(self,activation=tf.nn.relu):\n",
        "        self.X_s = tf.compat.v1.placeholder(tf.float32, [None, 2], name='X_s') # Input data\n",
        "        self.X_t = tf.compat.v1.placeholder(tf.float32, [None, 2], name='X_t')\n",
        "        self.Y_s = tf.compat.v1.placeholder(tf.int32, [None], name='Y_s')  # Class index\n",
        "        self.Y_t = tf.compat.v1.placeholder(tf.int32, [None], name='Y_t')\n",
        "        self.D_ind = tf.compat.v1.placeholder(tf.int32, [None], name='D_ind')  # Domain index\n",
        "        self.train = tf.compat.v1.placeholder(tf.bool, [], name='train')       # Switch for routing data to class predictor\n",
        "        self.l = tf.compat.v1.placeholder(tf.float32, [], name='l')        # Gradient reversal scaler\n",
        "\n",
        "        y_s = tf.one_hot(self.Y_s, 2)\n",
        "        y_t = tf.one_hot(self.Y_t, 2)\n",
        "\n",
        "        D = tf.one_hot(self.D_ind, 2)\n",
        "\n",
        "        # Feature extractor - single layer\n",
        "        with tf.compat.v1.variable_scope('source_feature_extractor'):\n",
        "            layer1=fc_layer(self.X_s,2,8, name=\"source_fc1\")\n",
        "            layer1=activation(layer1)\n",
        "            F_s=fc_layer(layer1,8,16,name=\"source_fc2\")\n",
        "            self.F_s=activation(F_s)\n",
        "        \n",
        "        with tf.compat.v1.variable_scope('target_feature_extractor'):\n",
        "            layer1_t=fc_layer(self.X_t,2,8, name=\"target_fc1\")\n",
        "            layer1_t=activation(layer1_t)\n",
        "            F_t=fc_layer(layer1_t,8,16,name=\"target_fc2\")\n",
        "            self.F_t=activation(F_t)\n",
        "        \n",
        "        with tf.compat.v1.variable_scope('label_predictor'):\n",
        "            # Label predictor - single layer\n",
        "            target_features = lambda: self.F_t\n",
        "            source_features = lambda: self.F_s\n",
        "            classify_feats = tf.cond(self.train, source_features, target_features)\n",
        "\n",
        "            target_labels = lambda:y_t\n",
        "            source_labels = lambda:y_s\n",
        "            self.classify_labels = tf.cond(self.train, source_labels, target_labels)\n",
        "            dim=classify_feats.get_shape().as_list()[1]\n",
        "          \n",
        "            p_logit=fc_layer(classify_feats,dim,2,name='fc3')\n",
        "            p = tf.nn.softmax(p_logit)\n",
        "            p_loss = tf.nn.softmax_cross_entropy_with_logits(logits=p_logit, labels=self.classify_labels )\n",
        "        \n",
        "\n",
        "        domain_merge=layers.concatenate([self.F_s,self.F_t],name='merged_features',axis=0)\n",
        "        self.feature=domain_merge\n",
        "\n",
        "        if self.loss=='MMD':\n",
        "            with tf.variable_scope('MMD'):\n",
        "                s_prop = fixprob(self.F_s)\n",
        "                t_prop = fixprob(self.F_t)\n",
        "               \n",
        "                self.adversarial_loss=tf.reduce_sum(mmd_loss(s_prop, t_prop,1))\n",
        "\n",
        "        elif self.loss=='DANN':\n",
        "\n",
        "            with tf.variable_scope('domain_predictor'):\n",
        "                feat = flip_gradient(self.feature,self.l)\n",
        "        \n",
        "                d=tf.compat.v1.layers.dense(feat,16,activation=activation)\n",
        "                d=tf.compat.v1.layers.dense(d,8,activation=activation)\n",
        "                d_logit=tf.compat.v1.layers.dense(d,2)\n",
        "                d = tf.nn.softmax(d_logit)\n",
        "                self.adversarial_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=d_logit, labels=D))\n",
        "\n",
        "\n",
        "\n",
        "        # Optimization\n",
        "\n",
        "        self.pred_loss = tf.reduce_mean(p_loss, name='pred_loss')\n",
        "        \n",
        "        self.total_loss = self.pred_loss+self.adversarial_loss\n",
        "\n",
        "        self.pred_train_op = tf.train.AdamOptimizer().minimize(self.pred_loss, name='pred_train_op')\n",
        "        self.domain_train_op = tf.train.AdamOptimizer().minimize(self.adversarial_loss, name='domain_train_op')\n",
        "        self.total_train_op = tf.train.AdamOptimizer().minimize(self.total_loss, name='dann_train_op')\n",
        "\n",
        "        # Evaluation\n",
        "        self.p_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.classify_labels, 1), tf.argmax(p, 1)), tf.float32), name='p_acc')\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87Xrj1fm44Sq"
      },
      "source": [
        "#%%\n",
        "tf.reset_default_graph()\n",
        "graph = tf.get_default_graph()\n",
        "\n",
        "with graph.as_default():\n",
        "    model = DANN('DANN',tf.nn.relu)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li9QputF4_JK"
      },
      "source": [
        "\n",
        "\n",
        "#%%  \n",
        "        \n",
        "def train_and_evaluate( grad_scale=None, num_batches=8000, verbose=True):\n",
        "    # Create batch builders\n",
        "    \n",
        "  S_batches = batch_generator([X_train_s, y_train_s], batch_size // 2)\n",
        "  T_batches = batch_generator([X_train_t, y_train_t], batch_size // 2)\n",
        "\n",
        "  \n",
        "  domain_labels = np.vstack([np.tile([1., 0.], [batch_size // 2, 1]),\n",
        "                                  np.tile([0., 1.], [batch_size // 2, 1])])\n",
        "  \n",
        "  # Get output tensors and train op\n",
        "  \n",
        "  p_acc = model.p_acc\n",
        "  train_loss = model.pred_loss\n",
        "  train_op = model.pred_train_op\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  for i in range(num_batches):\n",
        "\n",
        "    # If no grad_scale, use a schedule\n",
        "    if grad_scale is None:\n",
        "      p = float(i) / num_batches\n",
        "      lp = 2. / (1. + np.exp(-10. * p)) - 1\n",
        "    else:\n",
        "      lp = grad_scale\n",
        "\n",
        "    X0, y0 = next(S_batches)\n",
        "    X1, y1 = next(T_batches)\n",
        "    \n",
        "    D_labels = np.hstack([np.zeros(batch_size // 2, dtype=np.int32),np.ones(batch_size // 2, dtype=np.int32)])\n",
        "    _, loss, da, pa = sess.run([train_op, train_loss, model.adversarial_loss, p_acc],feed_dict={model.X_s: X0, model.X_t: X1, model.Y_s: y0,model.Y_t: y1, model.D_ind: D_labels,model.train: True, model.l: lp})\n",
        "\n",
        "    if verbose and i % 200 == 0:\n",
        "      print('loss: {}, domain accuracy: {}, class accuracy: {}'.format(loss, da, pa))\n",
        "\n",
        "            \n",
        "  # Get final accuracies on whole dataset\n",
        "  pas = sess.run(p_acc, feed_dict={model.X_s: X_test_s, model.X_t:X_test_t, model.Y_s: y_test_s,model.Y_t: y_test_t.shape,model.train: True, model.l: 1.0})\n",
        "  pat = sess.run(p_acc, feed_dict={model.X_s: X_test_s, model.X_t: X_test_t, model.Y_s: y_test_s,model.Y_t: y_test_t,model.train: False, model.l: 1.0})\n",
        "\n",
        "  #print('Source domain: ', das)\n",
        "  print('Source class: ', pas)\n",
        "  #print('Target domain: ', dat)\n",
        "  print('Target class: ', pat)\n",
        "  return sess,pas,pat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAEBdjurFxdr"
      },
      "source": [
        "def extract_and_plot_pca_feats(sess,source_acc,target_acc):\n",
        "    \n",
        "    emb_s = sess.run(model.F_s, feed_dict={model.X_s: X_test_s})\n",
        "    emb_t = sess.run(model.F_t, feed_dict={model.X_t: X_test_t})\n",
        "    emb_all = np.vstack([emb_s, emb_t])\n",
        "\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_emb = pca.fit_transform(emb_all)\n",
        "\n",
        "    num = pca_emb.shape[0] // 2\n",
        "    plt.scatter(pca_emb[:num,0], pca_emb[:num,1], c=y_test_s, cmap='coolwarm', alpha=0.4,label='source')\n",
        "    plt.scatter(pca_emb[num:,0], pca_emb[num:,1], c=y_test_t, cmap='cool', alpha=0.4,label='target')\n",
        "    #plt.legend(loc='best')\n",
        "    plt.title('Domain adaptation with hard parameter sharing',fontsize=14,fontweight='bold')\n",
        "    plt.xlabel('accuracy on source:%.2f \\n accuracy on target %.2f'%(source_acc,target_acc) ,fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWmEtpuN41yT"
      },
      "source": [
        "       \n",
        " #%%       \n",
        "sess,pas,pat=train_and_evaluate(verbose=True)\n",
        "extract_and_plot_pca_feats(sess,pas,pat)    \n",
        " \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaTlm0hO6KLd"
      },
      "source": [
        "#%%\n",
        "feature=[]\n",
        "for i in tf.trainable_variables(scope='source_feature_extractor'):\n",
        "    feature.append(sess.run(i))\n",
        "#%%\n",
        "label_class=[]\n",
        "for i in tf.trainable_variables(scope='label_predictor'):\n",
        "    label_class.append(sess.run(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4hWZehY4TfV"
      },
      "source": [
        "#%%\n",
        "batch_size = 16\n",
        "class SPS:\n",
        "    \"\"\"Simple soft parameter sharing (SPS) domain adaptation model.\"\"\"\n",
        "    def __init__(self,loss,activation):\n",
        "        self.loss=loss\n",
        "        self.build_model(activation)\n",
        "        \n",
        "    def build_model(self,activation=tf.nn.relu):\n",
        "        self.X_s = tf.compat.v1.placeholder(tf.float32, [None, 2], name='X_s') # Input data\n",
        "        self.X_t = tf.compat.v1.placeholder(tf.float32, [None, 2], name='X_t')\n",
        "        self.Y_s = tf.compat.v1.placeholder(tf.int32, [None], name='Y_s')  # Class index\n",
        "        self.Y_t = tf.compat.v1.placeholder(tf.int32, [None], name='Y_t')\n",
        "        self.D_ind = tf.compat.v1.placeholder(tf.int32, [None], name='D_ind')  # Domain index\n",
        "        self.train = tf.compat.v1.placeholder(tf.bool, [], name='train')       # Switch for routing data to class predictor\n",
        "        self.l = tf.compat.v1.placeholder(tf.float32, [], name='l')        # Gradient reversal scaler\n",
        "\n",
        "        y_s = tf.one_hot(self.Y_s, 2)\n",
        "        y_t = tf.one_hot(self.Y_t, 2)\n",
        "\n",
        "        D = tf.one_hot(self.D_ind, 2)\n",
        "\n",
        "        # Feature extractor - single layer\n",
        "        with tf.compat.v1.variable_scope('source_feature_extractor'):\n",
        "            layer1=fc_layer(self.X_s,2,8, name=\"source_fc1\",init_val=[feature[0],feature[1]])\n",
        "            layer1=activation(layer1)\n",
        "            F_s=fc_layer(layer1,8,16,name=\"source_fc2\",init_val=[feature[2],feature[3]])\n",
        "            self.F_s=activation(F_s)\n",
        "        \n",
        "        with tf.compat.v1.variable_scope('target_feature_extractor'):\n",
        "            layer1_t=fc_layer(self.X_t,2,8, name=\"target_fc1\",init_val=[feature[0],feature[1]])\n",
        "            layer1_t=activation(layer1_t)\n",
        "            F_t=fc_layer(layer1_t,8,16,name=\"target_fc2\",init_val=[feature[2],feature[3]])\n",
        "            self.F_t=activation(F_t)\n",
        "        \n",
        "        with tf.compat.v1.variable_scope('label_predictor'):\n",
        "            # Label predictor - single layer\n",
        "            target_features = lambda: self.F_t\n",
        "            source_features = lambda: self.F_s\n",
        "            classify_feats = tf.cond(self.train, source_features, target_features)\n",
        "\n",
        "            target_labels = lambda:y_t\n",
        "            source_labels = lambda:y_s\n",
        "            self.classify_labels = tf.cond(self.train, source_labels, target_labels)\n",
        "            dim=classify_feats.get_shape().as_list()[1]\n",
        "          \n",
        "            p_logit=fc_layer(classify_feats,dim,2,name='fc3',init_val=[label_class[0],label_class[1]],tr=False)\n",
        "            p = tf.nn.softmax(p_logit)\n",
        "            p_loss = tf.nn.softmax_cross_entropy_with_logits(logits=p_logit, labels=self.classify_labels )\n",
        "        \n",
        "\n",
        "        domain_merge=layers.concatenate([self.F_s,self.F_t],name='merged_features',axis=0)\n",
        "        self.feature=domain_merge\n",
        "\n",
        "        if self.loss=='MMD':\n",
        "            with tf.variable_scope('MMD'):\n",
        "                s_prop = fixprob(self.F_s)\n",
        "                t_prop = fixprob(self.F_t)\n",
        "               \n",
        "                self.MMD_loss=tf.reduce_sum(mmd_loss(s_prop, t_prop,1))\n",
        "                L2=sum(linear_model(tf_var1,tf_var2,'lin%s'%(index)) for index, (tf_var1,tf_var2) in enumerate(zip(tf.trainable_variables( scope='source_feature_extractor')[0:4],tf.trainable_variables( scope='target_feature_extractor')[0:4])))\n",
        "\n",
        "                self.pred_loss = tf.reduce_mean(p_loss, name='pred_loss')\n",
        "                self.total_loss = self.pred_loss+self.MMD_loss+0.5*L2\n",
        "                self.total_train_op = tf.train.AdamOptimizer().minimize(self.total_loss, name='mmd_train_op')\n",
        "                \n",
        "\n",
        "        elif self.loss=='DANN':\n",
        "\n",
        "            with tf.variable_scope('domain_predictor'):\n",
        "                feat = flip_gradient(self.feature,self.l)\n",
        "        \n",
        "                d=tf.compat.v1.layers.dense(feat,32,activation=activation)\n",
        "                d=tf.compat.v1.layers.dense(d,16,activation=activation)\n",
        "                d_logit=tf.compat.v1.layers.dense(d,2)\n",
        "                d = tf.nn.softmax(d_logit)\n",
        "                self.DC_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=d_logit, labels=D))\n",
        "                L2=sum(linear_model(tf_var1,tf_var2,'lin%s'%(index)) for index, (tf_var1,tf_var2) in enumerate(zip(tf.trainable_variables( scope='source_feature_extractor')[0:4],tf.trainable_variables( scope='target_feature_extractor')[0:4])))\n",
        "\n",
        "                self.pred_loss = tf.reduce_mean(p_loss, name='pred_loss')\n",
        "                self.total_loss = self.pred_loss+self.DC_loss+0.5*L2\n",
        "                self.total_train_op = tf.train.AdamOptimizer().minimize(self.total_loss, name='dann_train_op')\n",
        "                \n",
        "        \n",
        "        # elif self.loss=='both':\n",
        "        #     with tf.variable_scope('MMD'):\n",
        "        #         s_prop = fixprob(self.F_s)\n",
        "        #         t_prop = fixprob(self.F_t)\n",
        "               \n",
        "        #         self.MMD_loss=tf.reduce_sum(mmd_loss(s_prop, t_prop,1))\n",
        "\n",
        "        #     with tf.variable_scope('domain_predictor'):\n",
        "        #         feat = flip_gradient(self.feature,self.l)\n",
        "        \n",
        "        #         d=tf.compat.v1.layers.dense(feat,32,activation=activation)\n",
        "        #         d=tf.compat.v1.layers.dense(d,16,activation=activation)\n",
        "        #         d_logit=tf.compat.v1.layers.dense(d,2)\n",
        "        #         d = tf.nn.softmax(d_logit)\n",
        "        #         self.dann_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=d_logit, labels=D))\n",
        "\n",
        "\n",
        "\n",
        "        # Optimization\n",
        "\n",
        "\n",
        "        # Evaluation\n",
        "        self.p_acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.classify_labels, 1), tf.argmax(p, 1)), tf.float32), name='p_acc')\n",
        "\n",
        "        \n",
        "\n",
        "def linear_model(var1,var2,name):\n",
        "    \n",
        "    with tf.name_scope(name):\n",
        "        \n",
        "        var1=tf.reshape(var1,[-1])\n",
        "        n=var1.get_shape().as_list()[0]\n",
        "        var2=tf.reshape(var2,[-1])\n",
        "       \n",
        "        a= tf.Variable(tf.zeros(n), name=\"a\")\n",
        "        b=tf.Variable(tf.zeros(n), name=\"b\")\n",
        "        A=var1-var2-tf.keras.activations.tanh(a*var2+b)\n",
        "       \n",
        "        return tf.nn.l2_loss(A)       \n",
        " \n",
        "#%%\n",
        "\n",
        "sess.close()\n",
        "tf.reset_default_graph()\n",
        "graph = tf.get_default_graph()\n",
        "\n",
        "with graph.as_default():\n",
        "    model = SPS('MMD',tf.nn.relu)\n",
        "\n",
        "       \n",
        "def train_and_evaluate(grad_scale=None, num_batches=8000, verbose=True):\n",
        "    # Create batch builders\n",
        "    S_batches = batch_generator([X_train_s, y_train_s], batch_size // 2)\n",
        "    T_batches = batch_generator([X_train_t, y_train_t], batch_size // 2)\n",
        "\n",
        "    \n",
        "    domain_labels = np.vstack([np.tile([1., 0.], [batch_size // 2, 1]),\n",
        "                                   np.tile([0., 1.], [batch_size // 2, 1])])\n",
        "    \n",
        "    # Get output tensors and train op\n",
        "    \n",
        "    p_acc = model.p_acc\n",
        "    train_loss = model.pred_loss\n",
        "    train_op = model.total_train_op\n",
        "    sess=tf.Session()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(num_batches):\n",
        "\n",
        "        # If no grad_scale, use a schedule\n",
        "        if grad_scale is None:\n",
        "            p = float(i) / num_batches\n",
        "            lp = 2. / (1. + np.exp(-10. * p)) - 1\n",
        "        else:\n",
        "            lp = grad_scale\n",
        "\n",
        "        X0, y0 = next(S_batches)\n",
        "        X1, y1 = next(T_batches)\n",
        "        \n",
        "        D_labels = np.hstack([np.zeros(batch_size // 2, dtype=np.int32),\n",
        "                              np.ones(batch_size // 2, dtype=np.int32)])\n",
        "        _, loss, pa = sess.run([train_op, train_loss, p_acc],\n",
        "                                   feed_dict={model.X_s: X0, model.X_t: X1, model.Y_s: y0,model.Y_t: y1, model.D_ind: D_labels,\n",
        "                                              model.train: True, model.l: lp})\n",
        "\n",
        "        if verbose and i % 200 == 0:\n",
        "            print('loss: {}, class accuracy: {}'.format(loss, pa))\n",
        "\n",
        "            \n",
        "    # Get final accuracies on whole dataset\n",
        "    pas = sess.run(p_acc, feed_dict={model.X_s: X_test_s, model.X_t:X_test_t, model.Y_s: y_test_s,model.Y_t: y_test_t.shape, \n",
        "                            model.train: True, model.l: 1.0})\n",
        "    pat = sess.run(p_acc, feed_dict={model.X_s: X_test_s, model.X_t: X_test_t, model.Y_s: y_test_s,model.Y_t: y_test_t,\n",
        "                             model.train: False, model.l: 1.0})\n",
        "\n",
        "    #print('Source domain: ', das)\n",
        "    print('Source class: ', pas)\n",
        "    #print('Target domain: ', dat)\n",
        "    print('Target class: ', pat)\n",
        "    return sess, pas,pat\n",
        "\n",
        "#%%\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-Dc1CzXun0B"
      },
      "source": [
        "def extract_and_plot_pca_feats(sess,source_acc,target_acc):\n",
        "    \n",
        "    emb_s = sess.run(model.F_s, feed_dict={model.X_s: X_test_s})\n",
        "    emb_t = sess.run(model.F_t, feed_dict={model.X_t: X_test_t})\n",
        "    emb_all = np.vstack([emb_s, emb_t])\n",
        "\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_emb = pca.fit_transform(emb_all)\n",
        "\n",
        "    num = pca_emb.shape[0] // 2\n",
        "    plt.scatter(pca_emb[:num,0], pca_emb[:num,1], c=y_test_s, cmap='coolwarm', alpha=0.4,label='source')\n",
        "    plt.scatter(pca_emb[num:,0], pca_emb[num:,1], c=y_test_t, cmap='cool', alpha=0.4,label='target')\n",
        "    #plt.legend(loc='best')\n",
        "    plt.title('Domain adaptation with soft parameter sharing',fontsize=14,fontweight='bold')\n",
        "    plt.xlabel('accuracy on source:%.2f \\n accuracy on target %.2f'%(source_acc,target_acc) ,fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp_kqHhI6XKM"
      },
      "source": [
        "sess,pas,pat=train_and_evaluate( verbose=True)\n",
        "extract_and_plot_pca_feats(sess,pas,pat)  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}